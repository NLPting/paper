{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import nea.asap_reader as dataset \n",
    "from nea.asap_reader import read_org_dataset\n",
    "import nea.utils as U\n",
    "import os\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../data/fold_0/train.tsv'\n",
    "dev_path = '../../data/fold_0/dev.tsv'\n",
    "test_path = '../../data/fold_0/test.tsv'\n",
    "GLOVE_DIR = \"../../../embeddings/glove/\"\n",
    "out_dir = 'output'\n",
    "vocab_size = 4000\n",
    "maxlen = 0\n",
    "prompt_id = int(8)\n",
    "w = np.load('my_level_vocab.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_index = {}\n",
    "#f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'), encoding = 'utf-8')\n",
    "#for line in f:\n",
    "#    values = line.split()\n",
    "#    word = values[0]\n",
    "#    coefs = np.asarray(values[1:], dtype='float32')\n",
    "#    embeddings_index[word] = coefs\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix_glove = np.zeros((len(vocab), 50))\n",
    "#for word, i in vocab.items():\n",
    "#    embedding_vector = embeddings_index.get(word)\n",
    "#    if embedding_vector is not None:\n",
    "#        embedding_matrix_glove[i] = embedding_vector\n",
    "#print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix_glove, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# data_x is a list of lists\n",
    "(train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data(\n",
    "(train_path, dev_path, test_path), prompt_id, vocab_size, maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)\n",
    "dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y, dtype=K.floatx())\n",
    "dev_y = np.array(dev_y, dtype=K.floatx())\n",
    "test_y = np.array(test_y, dtype=K.floatx())\n",
    "train_pmt = np.array(train_pmt, dtype='int32')\n",
    "dev_pmt = np.array(dev_pmt, dtype='int32')\n",
    "test_pmt = np.array(test_pmt, dtype='int32')\n",
    "# We need the dev and test sets in the original scale for evaluation\n",
    "dev_y_org = dev_y.astype(dataset.get_ref_dtype())\n",
    "test_y_org = test_y.astype(dataset.get_ref_dtype())\n",
    "# Convert scores to boundary of [0 1] for training and evaluation (loss calculation)\n",
    "train_y = dataset.get_model_friendly_scores(train_y, train_pmt)\n",
    "dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)\n",
    "test_y = dataset.get_model_friendly_scores(test_y, test_pmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model , Input\n",
    "from keras.layers import Dense, Embedding, Activation , Flatten  , concatenate\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Dropout, GlobalAveragePooling1D ,GlobalMaxPooling1D , MaxPooling1D\n",
    "from keras.layers import LSTM , Bidirectional , Convolution1D\n",
    "from nea.optimizers import get_optimizer\n",
    "from dl_text import rd_ft\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org = read_org_dataset(train_path , prompt_id ,tokenize_text=True, to_lower=True )\n",
    "dev_org = read_org_dataset(dev_path , prompt_id ,tokenize_text=True, to_lower=True )\n",
    "test_org = read_org_dataset(test_path , prompt_id ,tokenize_text=True, to_lower=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lg_feature(org_array):\n",
    "    return len(org_array)\n",
    "def lg_level_feature(org_array):\n",
    "    return len([word for word in org_array if word in w])\n",
    "def differe_level_feature(org_array , delete_key=['A1']):\n",
    "    feature_map = {'A1':0,'A2':0,'B1':0,'B2':0,'C1':0,'C2':0}\n",
    "    for word in org_array:\n",
    "        if word in w:\n",
    "            if w[word]=='A1':feature_map['A1']+=1\n",
    "            if w[word]=='A2':feature_map['A2']+=1\n",
    "            if w[word]=='B1':feature_map['B1']+=1\n",
    "            if w[word]=='B2':feature_map['B2']+=1\n",
    "            if w[word]=='C1':feature_map['C1']+=1\n",
    "            if w[word]=='C2':feature_map['C2']+=1\n",
    "    #print(feature_map)\n",
    "    F = [feature_map[key] for key in feature_map.keys() if key not in delete_key]\n",
    "    return F  \n",
    "def get_feature(org_array):\n",
    "    corpus = ' '.join(org_array)\n",
    "    f1 = lg_feature(org_array)\n",
    "    f2 = lg_level_feature(org_array)\n",
    "    f3 = differe_level_feature(org_array)\n",
    "    f4 = textstat.flesch_reading_ease(corpus)\n",
    "    f5 = textstat.smog_index(corpus)\n",
    "    f6 = textstat.flesch_kincaid_grade(corpus)\n",
    "    f7 = textstat.coleman_liau_index(corpus)\n",
    "    f8 = textstat.automated_readability_index(corpus)\n",
    "    f9 = textstat.dale_chall_readability_score(corpus)\n",
    "    f10 = textstat.difficult_words(corpus)\n",
    "    f11 = textstat.linsear_write_formula(corpus)\n",
    "    f12 = textstat.gunning_fog(corpus)\n",
    "    #F = [f2,*f3,f6,f7,f8,f9,f10,f11,f12]\n",
    "    F = [f2,*f3 , f6  , f8 , f9 , f10]\n",
    "    return F \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = np.array([get_feature(x) for x in train_org] , dtype=K.floatx())\n",
    "dev_f =  np.array([get_feature(x) for x in dev_org] , dtype=K.floatx())\n",
    "test_f = np.array([get_feature(x) for x in test_org] , dtype=K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape=(overal_maxlen,), dtype='float64')\n",
    "embedder = Embedding(len(vocab), 50, input_length = overal_maxlen) \n",
    "#embedder = (Embedding(len(vocab),50,weights=[embedding_matrix_glove],input_length=overal_maxlen,trainable=True))\n",
    "embed = embedder(main_input)\n",
    "cnn1 = Convolution1D(filters=50 ,kernel_size=3)(embed)\n",
    "flat = Flatten()(cnn1)\n",
    "#global1 = GlobalAveragePooling1D()(cnn1)\n",
    "######################\n",
    "Other_Input = Input(shape=(len(train_f[0]),))\n",
    "add_feature = concatenate([flat, Other_Input],axis=1)\n",
    "##########################################\n",
    "add_feature = Dropout(0.5)(add_feature)\n",
    "main_output = Dense(1, activation='sigmoid')(add_feature)\n",
    "\n",
    "model = Model(inputs = [main_input , Other_Input], outputs = main_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mean_squared_error'\n",
    "metric = 'mean_absolute_error'\n",
    "opt = get_optimizer('adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_79 (InputLayer)           (None, 1122)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_40 (Embedding)        (None, 1122, 50)     200000      input_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 1120, 50)     7550        embedding_40[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_40 (Flatten)            (None, 56000)        0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_80 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 56010)        0           flatten_40[0][0]                 \n",
      "                                                                 input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 56010)        0           concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 1)            56011       dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 263,561\n",
      "Trainable params: 263,561\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dev]   loss: 0.0601, metric: 0.2202, mean: 24.314 (37.201), stdev: 2.999 (5.906)\n",
      "[Test]  loss: 0.0561, metric: 0.2134, mean: 24.551 (37.248), stdev: 2.852 (5.279)\n",
      "[DEV]   QWK:  -0.034, LWK: -0.014, PRS: -0.181, SPR: -0.263, Tau: -0.190 (Best @ -1: {{-0.034}}, -0.014, -0.181, -0.263, -0.190)\n",
      "[TEST]  QWK:  -0.024, LWK: -0.006, PRS: -0.153, SPR: -0.169, Tau: -0.123 (Best @ -1: {{-0.024}}, -0.006, -0.153, -0.169, -0.123)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nea.evalute_twoinput import Evaluator \n",
    "evl = Evaluator (dataset, prompt_id, out_dir, dev_x , dev_f  ,test_x , test_f , dev_y, test_y, dev_y_org, test_y_org)\n",
    "evl.evaluate(model, -1, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "434/434 [==============================] - 1s 2ms/step - loss: 0.0843 - mean_absolute_error: 0.2425\n",
      "Epoch 0, train: 1s, evaluation: 0s\n",
      "[Train] loss: 0.0843, metric: 0.2425\n",
      "[Dev]   loss: 0.0083, metric: 0.0712, mean: 34.269 (37.201), stdev: 4.988 (5.906)\n",
      "[Test]  loss: 0.0093, metric: 0.0773, mean: 33.487 (37.248), stdev: 5.508 (5.279)\n",
      "[DEV]   QWK:  0.564, LWK: 0.343, PRS: 0.652, SPR: 0.635, Tau: 0.477 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.534, LWK: 0.317, PRS: 0.672, SPR: 0.658, Tau: 0.493 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 204us/step - loss: 0.0223 - mean_absolute_error: 0.1190\n",
      "Epoch 1, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0223, metric: 0.1190\n",
      "[Dev]   loss: 0.0158, metric: 0.1047, mean: 42.736 (37.201), stdev: 5.339 (5.906)\n",
      "[Test]  loss: 0.0125, metric: 0.0924, mean: 41.886 (37.248), stdev: 5.843 (5.279)\n",
      "[DEV]   QWK:  0.395, LWK: 0.219, PRS: 0.588, SPR: 0.562, Tau: 0.419 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.464, LWK: 0.272, PRS: 0.622, SPR: 0.593, Tau: 0.437 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 219us/step - loss: 0.0101 - mean_absolute_error: 0.0818\n",
      "Epoch 2, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0101, metric: 0.0818\n",
      "[Dev]   loss: 0.0075, metric: 0.0685, mean: 38.874 (37.201), stdev: 1.974 (5.906)\n",
      "[Test]  loss: 0.0053, metric: 0.0599, mean: 38.472 (37.248), stdev: 2.207 (5.279)\n",
      "[DEV]   QWK:  0.348, LWK: 0.185, PRS: 0.620, SPR: 0.612, Tau: 0.451 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.440, LWK: 0.236, PRS: 0.652, SPR: 0.624, Tau: 0.464 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 195us/step - loss: 0.0047 - mean_absolute_error: 0.0540\n",
      "Epoch 3, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0047, metric: 0.0540\n",
      "[Dev]   loss: 0.0062, metric: 0.0600, mean: 37.645 (37.201), stdev: 3.412 (5.906)\n",
      "[Test]  loss: 0.0048, metric: 0.0557, mean: 37.046 (37.248), stdev: 3.798 (5.279)\n",
      "[DEV]   QWK:  0.530, LWK: 0.330, PRS: 0.606, SPR: 0.587, Tau: 0.436 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.587, LWK: 0.358, PRS: 0.624, SPR: 0.588, Tau: 0.435 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 198us/step - loss: 0.0022 - mean_absolute_error: 0.0364\n",
      "Epoch 4, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0022, metric: 0.0364\n",
      "[Dev]   loss: 0.0074, metric: 0.0656, mean: 35.168 (37.201), stdev: 2.252 (5.906)\n",
      "[Test]  loss: 0.0064, metric: 0.0631, mean: 34.713 (37.248), stdev: 2.544 (5.279)\n",
      "[DEV]   QWK:  0.399, LWK: 0.240, PRS: 0.658, SPR: 0.622, Tau: 0.462 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.441, LWK: 0.265, PRS: 0.664, SPR: 0.634, Tau: 0.473 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 197us/step - loss: 0.0020 - mean_absolute_error: 0.0343\n",
      "Epoch 5, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0020, metric: 0.0343\n",
      "[Dev]   loss: 0.0059, metric: 0.0599, mean: 37.934 (37.201), stdev: 3.292 (5.906)\n",
      "[Test]  loss: 0.0045, metric: 0.0539, mean: 37.353 (37.248), stdev: 3.717 (5.279)\n",
      "[DEV]   QWK:  0.540, LWK: 0.317, PRS: 0.645, SPR: 0.633, Tau: 0.474 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.609, LWK: 0.374, PRS: 0.648, SPR: 0.632, Tau: 0.468 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 197us/step - loss: 0.0020 - mean_absolute_error: 0.0353\n",
      "Epoch 6, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0020, metric: 0.0353\n",
      "[Dev]   loss: 0.0073, metric: 0.0648, mean: 35.132 (37.201), stdev: 2.559 (5.906)\n",
      "[Test]  loss: 0.0065, metric: 0.0639, mean: 34.686 (37.248), stdev: 2.853 (5.279)\n",
      "[DEV]   QWK:  0.425, LWK: 0.263, PRS: 0.639, SPR: 0.612, Tau: 0.453 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.451, LWK: 0.271, PRS: 0.642, SPR: 0.620, Tau: 0.458 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 198us/step - loss: 0.0024 - mean_absolute_error: 0.0376\n",
      "Epoch 7, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0024, metric: 0.0376\n",
      "[Dev]   loss: 0.0059, metric: 0.0599, mean: 37.136 (37.201), stdev: 3.098 (5.906)\n",
      "[Test]  loss: 0.0047, metric: 0.0548, mean: 36.604 (37.248), stdev: 3.430 (5.279)\n",
      "[DEV]   QWK:  0.522, LWK: 0.308, PRS: 0.633, SPR: 0.605, Tau: 0.452 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.570, LWK: 0.346, PRS: 0.641, SPR: 0.617, Tau: 0.455 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 195us/step - loss: 0.0014 - mean_absolute_error: 0.0274\n",
      "Epoch 8, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0014, metric: 0.0274\n",
      "[Dev]   loss: 0.0063, metric: 0.0606, mean: 36.046 (37.201), stdev: 3.155 (5.906)\n",
      "[Test]  loss: 0.0055, metric: 0.0589, mean: 35.519 (37.248), stdev: 3.508 (5.279)\n",
      "[DEV]   QWK:  0.503, LWK: 0.311, PRS: 0.629, SPR: 0.610, Tau: 0.456 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.542, LWK: 0.321, PRS: 0.635, SPR: 0.615, Tau: 0.452 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 198us/step - loss: 0.0015 - mean_absolute_error: 0.0294\n",
      "Epoch 9, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0015, metric: 0.0294\n",
      "[Dev]   loss: 0.0074, metric: 0.0656, mean: 35.129 (37.201), stdev: 2.488 (5.906)\n",
      "[Test]  loss: 0.0065, metric: 0.0641, mean: 34.717 (37.248), stdev: 2.774 (5.279)\n",
      "[DEV]   QWK:  0.407, LWK: 0.245, PRS: 0.632, SPR: 0.603, Tau: 0.449 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.443, LWK: 0.263, PRS: 0.639, SPR: 0.623, Tau: 0.463 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 197us/step - loss: 0.0020 - mean_absolute_error: 0.0348\n",
      "Epoch 10, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0020, metric: 0.0348\n",
      "[Dev]   loss: 0.0064, metric: 0.0618, mean: 38.367 (37.201), stdev: 3.666 (5.906)\n",
      "[Test]  loss: 0.0049, metric: 0.0573, mean: 37.801 (37.248), stdev: 4.088 (5.279)\n",
      "[DEV]   QWK:  0.531, LWK: 0.325, PRS: 0.616, SPR: 0.593, Tau: 0.447 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.608, LWK: 0.355, PRS: 0.630, SPR: 0.609, Tau: 0.449 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "434/434 [==============================] - 0s 204us/step - loss: 0.0016 - mean_absolute_error: 0.0301\n",
      "Epoch 11, train: 0s, evaluation: 0s\n",
      "[Train] loss: 0.0016, metric: 0.0301\n",
      "[Dev]   loss: 0.0071, metric: 0.0638, mean: 35.588 (37.201), stdev: 2.612 (5.906)\n",
      "[Test]  loss: 0.0061, metric: 0.0622, mean: 35.146 (37.248), stdev: 2.947 (5.279)\n",
      "[DEV]   QWK:  0.419, LWK: 0.250, PRS: 0.609, SPR: 0.572, Tau: 0.426 (Best @ 0: {{0.564}}, 0.343, 0.652, 0.635, 0.477)\n",
      "[TEST]  QWK:  0.471, LWK: 0.274, PRS: 0.613, SPR: 0.584, Tau: 0.425 (Best @ 0: {{0.534}}, 0.317, 0.672, 0.658, 0.493)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "320/434 [=====================>........] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0346"
     ]
    }
   ],
   "source": [
    "total_train_time = 0\n",
    "total_eval_time = 0\n",
    "best , count = 0 , 0\n",
    "for ii in range(50):\n",
    "    # Training\n",
    "    t0 = time()\n",
    "    train_history = model.fit([train_x , train_f], train_y, batch_size=32, epochs=1, verbose=1)\n",
    "    #train_history = model.fit([train_f], train_y, batch_size=32, epochs=1, verbose=1)\n",
    "    tr_time = time() - t0\n",
    "    total_train_time += tr_time\n",
    "    # Evaluate\n",
    "    t0 = time()\n",
    "    evl.evaluate(model, ii)\n",
    "    evl_time = time() - t0\n",
    "    total_eval_time += evl_time\n",
    "    train_loss = train_history.history['loss'][0]\n",
    "    train_metric = train_history.history[metric][0]\n",
    "    print('Epoch %d, train: %is, evaluation: %is' % (ii, tr_time, evl_time))\n",
    "    print('[Train] loss: %.4f, metric: %.4f' % (train_loss, train_metric))\n",
    "    evl.print_info()\n",
    "    \n",
    "evl.print_final_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_no_wordvec_cnn_dim50 = (0.827+0.806+0.817+0.822+0.835)/5\n",
    "#base_no_wordvec_cnn_dim50_f3 = (0.812 + )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.842 + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmoblistm",
   "language": "python",
   "name": "elmoblistm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
