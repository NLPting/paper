{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import nea.asap_reader as dataset \n",
    "from nea.asap_reader import read_org_dataset\n",
    "import nea.utils as U\n",
    "import os\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"1\"\n",
    "set_session(tf.Session(config=config))\n",
    "import nlp_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../data/fold_0/train.tsv'\n",
    "dev_path = '../../data/fold_0/dev.tsv'\n",
    "test_path = '../../data/fold_0/test.tsv'\n",
    "GLOVE_DIR = \"../../../embeddings/glove/\"\n",
    "out_dir = 'output'\n",
    "vocab_size = 4000\n",
    "maxlen = 0\n",
    "prompt_id = int(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# data_x is a list of lists\n",
    "(train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data(\n",
    "(train_path, dev_path, test_path), prompt_id, vocab_size, maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)\n",
    "dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y, dtype=K.floatx())\n",
    "dev_y = np.array(dev_y, dtype=K.floatx())\n",
    "test_y = np.array(test_y, dtype=K.floatx())\n",
    "train_pmt = np.array(train_pmt, dtype='int32')\n",
    "dev_pmt = np.array(dev_pmt, dtype='int32')\n",
    "test_pmt = np.array(test_pmt, dtype='int32')\n",
    "# We need the dev and test sets in the original scale for evaluation\n",
    "dev_y_org = dev_y.astype(dataset.get_ref_dtype())\n",
    "test_y_org = test_y.astype(dataset.get_ref_dtype())\n",
    "# Convert scores to boundary of [0 1] for training and evaluation (loss calculation)\n",
    "train_y = dataset.get_model_friendly_scores(train_y, train_pmt)\n",
    "dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)\n",
    "test_y = dataset.get_model_friendly_scores(test_y, test_pmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model , Input\n",
    "from keras.layers import Dense, Embedding, Activation , Flatten  , concatenate , BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Dropout, GlobalAveragePooling1D ,GlobalMaxPooling1D , MaxPooling1D , AveragePooling1D , GlobalMaxPooling1D\n",
    "from keras.layers import LSTM , Bidirectional , Convolution1D\n",
    "from nea.optimizers import get_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org = read_org_dataset(train_path , prompt_id ,tokenize_text=True, to_lower=True )\n",
    "dev_org = read_org_dataset(dev_path , prompt_id ,tokenize_text=True, to_lower=True )\n",
    "test_org = read_org_dataset(test_path , prompt_id ,tokenize_text=True, to_lower=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(org_array):\n",
    "    corpus = ' '.join(org_array)\n",
    "    f1 = nlp_feature.differe_level_feature(org_array)\n",
    "    f2 = nlp_feature.corpus_word_level_score(org_array)\n",
    "    f3 = nlp_feature.avg_sentence_length(corpus)\n",
    "    f4 = nlp_feature.punct_feature(corpus)\n",
    "    f5 = nlp_feature.avg_word_length(corpus)\n",
    "    f6 = nlp_feature.count_connectives(corpus)\n",
    "    f7 = nlp_feature.char_count(corpus)\n",
    "    f8 = nlp_feature.lexicon_count(corpus)\n",
    "    f9 = nlp_feature.flesch_reading_ease(corpus)\n",
    "    f10 = nlp_feature.smog_index(corpus)\n",
    "    f11 = nlp_feature.automated_readability_index(corpus)\n",
    "    #F = [*f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11]\n",
    "    F = [*f1 , f2 , f5 , f6 ,f7 , f8]\n",
    "    return F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler.fit(get_feature(train_org[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = np.array([get_feature(x) for x in train_org] , dtype=K.floatx())\n",
    "dev_f =  np.array([get_feature(x) for x in dev_org] , dtype=K.floatx())\n",
    "test_f = np.array([get_feature(x) for x in test_org] , dtype=K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(np.concatenate((train_f,dev_f,test_f), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = np.array(scaler.transform(train_f) , dtype=K.floatx())\n",
    "dev_f = np.array(scaler.transform(dev_f) , dtype=K.floatx())\n",
    "test_f = np.array(scaler.transform(test_f) , dtype=K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape=(overal_maxlen,), dtype='float64')\n",
    "embedder = Embedding(len(vocab), 50, input_length = overal_maxlen) \n",
    "embed = embedder(main_input)\n",
    "cnn1 = Convolution1D(filters=30 ,kernel_size=3)(embed)\n",
    "cnn1 = MaxPooling1D()(cnn1)\n",
    "flat = Flatten()(cnn1)\n",
    "#global1 = GlobalAveragePooling1D()(cnn1)\n",
    "######################\n",
    "Other_Input = Input(shape=(len(train_f[0]),))\n",
    "add_feature = concatenate([flat, Other_Input],axis=1)\n",
    "##########################################\n",
    "main_output = Dense(1, activation='sigmoid')(add_feature)\n",
    "\n",
    "model = Model(inputs = [main_input , Other_Input], outputs = main_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mean_squared_error'\n",
    "metric = 'mean_absolute_error'\n",
    "opt = get_optimizer('adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_160 (InputLayer)          (None, 423)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_81 (Embedding)        (None, 423, 50)      200000      input_160[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, 421, 30)      4530        embedding_81[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 210, 30)      0           conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_65 (Flatten)            (None, 6300)         0           max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_161 (InputLayer)          (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 6309)         0           flatten_65[0][0]                 \n",
      "                                                                 input_161[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 1)            6310        concatenate_69[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 210,840\n",
      "Trainable params: 210,840\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dev]   loss: 0.0915, metric: 0.2622, mean: 1.481 (1.867), stdev: 0.033 (0.813)\n",
      "[Test]  loss: 0.0892, metric: 0.2588, mean: 1.483 (1.821), stdev: 0.030 (0.820)\n",
      "[DEV]   QWK:  -0.106, LWK: -0.119, PRS: -0.249, SPR: -0.248, Tau: -0.193 (Best @ -1: {{-0.106}}, -0.119, -0.249, -0.248, -0.193)\n",
      "[TEST]  QWK:  -0.172, LWK: -0.166, PRS: -0.299, SPR: -0.300, Tau: -0.229 (Best @ -1: {{-0.172}}, -0.166, -0.299, -0.300, -0.229)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nea.evalute_twoinput import Evaluator \n",
    "evl = Evaluator (dataset, prompt_id, out_dir, dev_x , dev_f  ,test_x , test_f , dev_y, test_y, dev_y_org, test_y_org)\n",
    "evl.evaluate(model, -1, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "total_train_time = 0\n",
    "total_eval_time = 0\n",
    "best , count = 0 , 0\n",
    "for ii in range(50):\n",
    "    # Training\n",
    "    t0 = time()\n",
    "    train_history = model.fit([train_x , train_f], train_y, batch_size=32, epochs=1, verbose=1)\n",
    "    #train_history = model.fit([train_f], train_y, batch_size=32, epochs=1, verbose=1)\n",
    "    tr_time = time() - t0\n",
    "    total_train_time += tr_time\n",
    "    # Evaluate\n",
    "    t0 = time()\n",
    "    evl.evaluate(model, ii)\n",
    "    evl_time = time() - t0\n",
    "    total_eval_time += evl_time\n",
    "    train_loss = train_history.history['loss'][0]\n",
    "    train_metric = train_history.history[metric][0]\n",
    "    print('Epoch %d, train: %is, evaluation: %is' % (ii, tr_time, evl_time))\n",
    "    print('[Train] loss: %.4f, metric: %.4f' % (train_loss, train_metric))\n",
    "    evl.print_info()\n",
    "    \n",
    "evl.print_final_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [*f1 , f2 , f5 , f6 ,f7 , f8]\n",
    "0.854"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmoblistm",
   "language": "python",
   "name": "elmoblistm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
