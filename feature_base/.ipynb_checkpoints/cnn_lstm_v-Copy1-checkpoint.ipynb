{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/ting/anaconda3/envs/elmobilstm/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import nea.asap_reader as dataset \n",
    "from nea.asap_reader import read_org_dataset\n",
    "import nea.utils as U\n",
    "import os\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../data/fold_0/train.tsv'\n",
    "dev_path = '../../data/fold_0/dev.tsv'\n",
    "test_path = '../../data/fold_0/test.tsv'\n",
    "GLOVE_DIR = \"../../../embeddings/glove/\"\n",
    "out_dir = 'output'\n",
    "vocab_size = 4000\n",
    "maxlen = 0\n",
    "prompt_id = int(1)\n",
    "w = np.load('my_level_vocab.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_index = {}\n",
    "#f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'), encoding = 'utf-8')\n",
    "#for line in f:\n",
    "#    values = line.split()\n",
    "#    word = values[0]\n",
    "#    coefs = np.asarray(values[1:], dtype='float32')\n",
    "#    embeddings_index[word] = coefs\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix_glove = np.zeros((len(vocab), 50))\n",
    "#for word, i in vocab.items():\n",
    "#    embedding_vector = embeddings_index.get(word)\n",
    "#    if embedding_vector is not None:\n",
    "#        embedding_matrix_glove[i] = embedding_vector\n",
    "#print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix_glove, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# data_x is a list of lists\n",
    "(train_x, train_y, train_pmt), (dev_x, dev_y, dev_pmt), (test_x, test_y, test_pmt), vocab, vocab_size, overal_maxlen, num_outputs = dataset.get_data(\n",
    "(train_path, dev_path, test_path), prompt_id, vocab_size, maxlen, tokenize_text=True, to_lower=True, sort_by_len=False, vocab_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)\n",
    "dev_x = sequence.pad_sequences(dev_x, maxlen=overal_maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y, dtype=K.floatx())\n",
    "dev_y = np.array(dev_y, dtype=K.floatx())\n",
    "test_y = np.array(test_y, dtype=K.floatx())\n",
    "train_pmt = np.array(train_pmt, dtype='int32')\n",
    "dev_pmt = np.array(dev_pmt, dtype='int32')\n",
    "test_pmt = np.array(test_pmt, dtype='int32')\n",
    "# We need the dev and test sets in the original scale for evaluation\n",
    "dev_y_org = dev_y.astype(dataset.get_ref_dtype())\n",
    "test_y_org = test_y.astype(dataset.get_ref_dtype())\n",
    "# Convert scores to boundary of [0 1] for training and evaluation (loss calculation)\n",
    "train_y = dataset.get_model_friendly_scores(train_y, train_pmt)\n",
    "dev_y = dataset.get_model_friendly_scores(dev_y, dev_pmt)\n",
    "test_y = dataset.get_model_friendly_scores(test_y, test_pmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model , Input\n",
    "from keras.layers import Dense, Embedding, Activation , Flatten  , concatenate\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Dropout, GlobalAveragePooling1D ,GlobalMaxPooling1D , MaxPooling1D\n",
    "from keras.layers import LSTM , Bidirectional , Convolution1D\n",
    "from nea.optimizers import get_optimizer\n",
    "from dl_text import rd_ft\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org = read_org_dataset(train_path , prompt_id ,tokenize_text=True, to_lower=True )\n",
    "dev_org = read_org_dataset(dev_path , prompt_id ,tokenize_text=True, to_lower=True )\n",
    "test_org = read_org_dataset(test_path , prompt_id ,tokenize_text=True, to_lower=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lg_feature(org_array):\n",
    "    return len(org_array)\n",
    "def lg_level_feature(org_array):\n",
    "    return len([word for word in org_array if word in w])\n",
    "def differe_level_feature(org_array , delete_key=['A1']):\n",
    "    feature_map = {'A1':0,'A2':0,'B1':0,'B2':0,'C1':0,'C2':0}\n",
    "    for word in org_array:\n",
    "        if word in w:\n",
    "            if w[word]=='A1':feature_map['A1']+=1\n",
    "            if w[word]=='A2':feature_map['A2']+=1\n",
    "            if w[word]=='B1':feature_map['B1']+=1\n",
    "            if w[word]=='B2':feature_map['B2']+=1\n",
    "            if w[word]=='C1':feature_map['C1']+=1\n",
    "            if w[word]=='C2':feature_map['C2']+=1\n",
    "    #print(feature_map)\n",
    "    F = [feature_map[key] for key in feature_map.keys() if key not in delete_key]\n",
    "    return F  \n",
    "def get_feature(org_array):\n",
    "    corpus = ' '.join(org_array)\n",
    "    f1 = lg_feature(org_array)\n",
    "    f2 = lg_level_feature(org_array)\n",
    "    f3 = differe_level_feature(org_array)\n",
    "    f4 = textstat.flesch_reading_ease(corpus)\n",
    "    f5 = textstat.smog_index(corpus)\n",
    "    f6 = textstat.flesch_kincaid_grade(corpus)\n",
    "    f7 = textstat.coleman_liau_index(corpus)\n",
    "    f8 = textstat.automated_readability_index(corpus)\n",
    "    f9 = textstat.dale_chall_readability_score(corpus)\n",
    "    f10 = textstat.difficult_words(corpus)\n",
    "    f11 = textstat.linsear_write_formula(corpus)\n",
    "    f12 = textstat.gunning_fog(corpus)\n",
    "    F = [f2,*f3,f6,f7,f8,f9,f10,f11,f12]\n",
    "    #F = [f2,*f3 , f6  , f8 , f9 , f10]\n",
    "    #F = [f2,*f3]\n",
    "    return F \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = np.array([get_feature(x) for x in train_org] , dtype=K.floatx())\n",
    "dev_f =  np.array([get_feature(x) for x in dev_org] , dtype=K.floatx())\n",
    "test_f = np.array([get_feature(x) for x in test_org] , dtype=K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape=(overal_maxlen,), dtype='float64')\n",
    "embedder = Embedding(len(vocab), 50, input_length = overal_maxlen) \n",
    "#embedder = (Embedding(len(vocab),50,weights=[embedding_matrix_glove],input_length=overal_maxlen,trainable=True))\n",
    "embed = embedder(main_input)\n",
    "cnn1 = Convolution1D(filters=50 ,kernel_size=3)(embed)\n",
    "flat_cnn = Flatten()(cnn1)\n",
    "#######################################################\n",
    "lstm1 = LSTM(64 , return_sequences=True)(embed)\n",
    "flat_lstm = Flatten()(lstm1)\n",
    "\n",
    "######################################################\n",
    "Other_Input = Input(shape=(len(train_f[0]),))\n",
    "add_feature = concatenate([flat_cnn,flat_lstm, Other_Input],axis=1)\n",
    "##########################################\n",
    "#add_feature = Dropout(0.5)(add_feature)\n",
    "main_output = Dense(1, activation='sigmoid')(add_feature)\n",
    "\n",
    "model = Model(inputs = [main_input , Other_Input], outputs = main_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mean_squared_error'\n",
    "metric = 'mean_absolute_error'\n",
    "opt = get_optimizer('adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 939)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 939, 50)      200000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 937, 50)      7550        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 939, 64)      29440       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 46850)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 60096)        0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 106959)       0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            106960      concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 343,950\n",
      "Trainable params: 343,950\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dev]   loss: 0.1498, metric: 0.3403, mean: 5.303 (8.508), stdev: 0.776 (1.539)\n",
      "[Test]  loss: 0.1465, metric: 0.3397, mean: 5.328 (8.473), stdev: 0.776 (1.538)\n",
      "[DEV]   QWK:  -0.124, LWK: -0.032, PRS: -0.727, SPR: -0.715, Tau: -0.573 (Best @ -1: {{-0.124}}, -0.032, -0.727, -0.715, -0.573)\n",
      "[TEST]  QWK:  -0.129, LWK: -0.047, PRS: -0.748, SPR: -0.711, Tau: -0.573 (Best @ -1: {{-0.129}}, -0.047, -0.748, -0.711, -0.573)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nea.evalute_twoinput import Evaluator \n",
    "evl = Evaluator (dataset, prompt_id, out_dir, dev_x , dev_f  ,test_x , test_f , dev_y, test_y, dev_y_org, test_y_org)\n",
    "evl.evaluate(model, -1, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1070/1070 [==============================] - 34s 32ms/step - loss: 0.0160 - mean_absolute_error: 0.0888\n",
      "Epoch 0, train: 34s, evaluation: 3s\n",
      "[Train] loss: 0.0160, metric: 0.0888\n",
      "[Dev]   loss: 0.0076, metric: 0.0697, mean: 8.145 (8.508), stdev: 1.471 (1.539)\n",
      "[Test]  loss: 0.0081, metric: 0.0709, mean: 8.088 (8.473), stdev: 1.454 (1.538)\n",
      "[DEV]   QWK:  0.814, LWK: 0.598, PRS: 0.861, SPR: 0.844, Tau: 0.711 (Best @ 0: {{0.814}}, 0.598, 0.861, 0.844, 0.711)\n",
      "[TEST]  QWK:  0.809, LWK: 0.594, PRS: 0.853, SPR: 0.838, Tau: 0.702 (Best @ 0: {{0.809}}, 0.594, 0.853, 0.838, 0.702)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "1070/1070 [==============================] - 31s 29ms/step - loss: 0.0052 - mean_absolute_error: 0.0578\n",
      "Epoch 1, train: 31s, evaluation: 2s\n",
      "[Train] loss: 0.0052, metric: 0.0578\n",
      "[Dev]   loss: 0.0076, metric: 0.0682, mean: 8.123 (8.508), stdev: 1.210 (1.539)\n",
      "[Test]  loss: 0.0080, metric: 0.0693, mean: 8.082 (8.473), stdev: 1.188 (1.538)\n",
      "[DEV]   QWK:  0.788, LWK: 0.571, PRS: 0.864, SPR: 0.838, Tau: 0.703 (Best @ 0: {{0.814}}, 0.598, 0.861, 0.844, 0.711)\n",
      "[TEST]  QWK:  0.776, LWK: 0.569, PRS: 0.858, SPR: 0.844, Tau: 0.705 (Best @ 0: {{0.809}}, 0.594, 0.853, 0.838, 0.702)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "1070/1070 [==============================] - 31s 29ms/step - loss: 0.0023 - mean_absolute_error: 0.0371\n",
      "Epoch 2, train: 31s, evaluation: 2s\n",
      "[Train] loss: 0.0023, metric: 0.0371\n",
      "[Dev]   loss: 0.0069, metric: 0.0655, mean: 8.789 (8.508), stdev: 1.324 (1.539)\n",
      "[Test]  loss: 0.0069, metric: 0.0650, mean: 8.750 (8.473), stdev: 1.307 (1.538)\n",
      "[DEV]   QWK:  0.817, LWK: 0.612, PRS: 0.861, SPR: 0.840, Tau: 0.706 (Best @ 2: {{0.817}}, 0.612, 0.861, 0.840, 0.706)\n",
      "[TEST]  QWK:  0.818, LWK: 0.618, PRS: 0.861, SPR: 0.840, Tau: 0.702 (Best @ 2: {{0.818}}, 0.618, 0.861, 0.840, 0.702)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "1070/1070 [==============================] - 33s 31ms/step - loss: 0.0010 - mean_absolute_error: 0.0237\n",
      "Epoch 3, train: 33s, evaluation: 4s\n",
      "[Train] loss: 0.0010, metric: 0.0237\n",
      "[Dev]   loss: 0.0062, metric: 0.0614, mean: 8.631 (8.508), stdev: 1.364 (1.539)\n",
      "[Test]  loss: 0.0062, metric: 0.0611, mean: 8.590 (8.473), stdev: 1.348 (1.538)\n",
      "[DEV]   QWK:  0.840, LWK: 0.642, PRS: 0.864, SPR: 0.842, Tau: 0.708 (Best @ 3: {{0.840}}, 0.642, 0.864, 0.842, 0.708)\n",
      "[TEST]  QWK:  0.838, LWK: 0.648, PRS: 0.861, SPR: 0.838, Tau: 0.701 (Best @ 3: {{0.838}}, 0.648, 0.861, 0.838, 0.701)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "1070/1070 [==============================] - 35s 33ms/step - loss: 5.6692e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 4, train: 35s, evaluation: 2s\n",
      "[Train] loss: 0.0006, metric: 0.0162\n",
      "[Dev]   loss: 0.0060, metric: 0.0605, mean: 8.461 (8.508), stdev: 1.382 (1.539)\n",
      "[Test]  loss: 0.0062, metric: 0.0612, mean: 8.417 (8.473), stdev: 1.367 (1.538)\n",
      "[DEV]   QWK:  0.850, LWK: 0.656, PRS: 0.865, SPR: 0.843, Tau: 0.709 (Best @ 4: {{0.850}}, 0.656, 0.865, 0.843, 0.709)\n",
      "[TEST]  QWK:  0.840, LWK: 0.646, PRS: 0.861, SPR: 0.838, Tau: 0.700 (Best @ 4: {{0.840}}, 0.646, 0.861, 0.838, 0.700)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "1070/1070 [==============================] - 35s 32ms/step - loss: 3.7324e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 5, train: 34s, evaluation: 2s\n",
      "[Train] loss: 0.0004, metric: 0.0126\n",
      "[Dev]   loss: 0.0062, metric: 0.0617, mean: 8.671 (8.508), stdev: 1.371 (1.539)\n",
      "[Test]  loss: 0.0063, metric: 0.0614, mean: 8.627 (8.473), stdev: 1.356 (1.538)\n",
      "[DEV]   QWK:  0.847, LWK: 0.650, PRS: 0.866, SPR: 0.842, Tau: 0.708 (Best @ 4: {{0.850}}, 0.656, 0.865, 0.843, 0.709)\n",
      "[TEST]  QWK:  0.848, LWK: 0.659, PRS: 0.863, SPR: 0.839, Tau: 0.701 (Best @ 4: {{0.840}}, 0.646, 0.861, 0.838, 0.700)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "1070/1070 [==============================] - 32s 30ms/step - loss: 3.9713e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 6, train: 31s, evaluation: 2s\n",
      "[Train] loss: 0.0004, metric: 0.0135\n",
      "[Dev]   loss: 0.0060, metric: 0.0603, mean: 8.414 (8.508), stdev: 1.358 (1.539)\n",
      "[Test]  loss: 0.0062, metric: 0.0616, mean: 8.365 (8.473), stdev: 1.338 (1.538)\n",
      "[DEV]   QWK:  0.855, LWK: 0.664, PRS: 0.867, SPR: 0.840, Tau: 0.705 (Best @ 6: {{0.855}}, 0.664, 0.867, 0.840, 0.705)\n",
      "[TEST]  QWK:  0.833, LWK: 0.638, PRS: 0.861, SPR: 0.837, Tau: 0.699 (Best @ 6: {{0.833}}, 0.638, 0.861, 0.837, 0.699)\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1/1\n",
      "  32/1070 [..............................] - ETA: 30s - loss: 6.9791e-04 - mean_absolute_error: 0.0217"
     ]
    }
   ],
   "source": [
    "total_train_time = 0\n",
    "total_eval_time = 0\n",
    "best , count = 0 , 0\n",
    "for ii in range(50):\n",
    "    # Training\n",
    "    t0 = time()\n",
    "    train_history = model.fit([train_x , train_f], train_y, batch_size=32, epochs=1, verbose=1)\n",
    "    #train_history = model.fit([train_f], train_y, batch_size=32, epochs=1, verbose=1)\n",
    "    tr_time = time() - t0\n",
    "    total_train_time += tr_time\n",
    "    # Evaluate\n",
    "    t0 = time()\n",
    "    evl.evaluate(model, ii)\n",
    "    evl_time = time() - t0\n",
    "    total_eval_time += evl_time\n",
    "    train_loss = train_history.history['loss'][0]\n",
    "    train_metric = train_history.history[metric][0]\n",
    "    print('Epoch %d, train: %is, evaluation: %is' % (ii, tr_time, evl_time))\n",
    "    print('[Train] loss: %.4f, metric: %.4f' % (train_loss, train_metric))\n",
    "    evl.print_info()\n",
    "    \n",
    "evl.print_final_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_no_wordvec_cnn_dim50 = (0.827+0.806+0.817+0.822+0.835)/5\n",
    "#base_no_wordvec_cnn_dim50_f3 = (0.812 + )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.842 + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmoblistm",
   "language": "python",
   "name": "elmoblistm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
